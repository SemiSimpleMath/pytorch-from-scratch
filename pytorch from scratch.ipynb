{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a training loop from scratch.\n",
    "\n",
    "The aim of this project is to recreate the Pytorch nn.model and write all training code from scratch only using basic numpy stuff.  This project is inspired by fast.ai course where they rewrite much of pytorch.  Although Some of the code comes directly from fast.ai, I am writing everything in my own style (except where it made no sense to reinvent the wheel).  Furthermore, I am diving lot deeper into some topics that fast.ai just mskimmed over.  In particular I explain in detail the math underlying backpropagation and discuss cross entropy and its gradient.  This project to me was tremendously useful.  Although I have trained many neural nets in various projects I have always used full machinery of pytorch.  \n",
    "\n",
    "- I now undertstand pytorch and neural nets in much deeper way.\n",
    "- I always knew that backpropagation was just a chain rule, but now I understand how to implement it and all little details are now clear.  e.g. Why is it that in a linear layer the outgoing grad is multiplied by the tranpose of the weight matrix.\n",
    "- I see the beuty and simplicity in the implementation of the backpropagation where we just have to keep track of inp.g and out.g for each layer and how to compute each of them.\n",
    "- This project brought forth a lot of tricky concepts like broadcasting.  It also made taught me the importance of initializing weights properly.  (kaiming init.)  \n",
    "- I had to consider all sorts of computational tricks so that values would not \"blow up\".  This was particularly the case in computation of softmax and cross entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\pytorch from scartch\\data\\mnist.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "#Get MNIST data\n",
    "MNIST_PATH='D:\\\\pytorch from scartch\\data\\\\'\n",
    "MNIST_FILE = MNIST_PATH + \"mnist.pkl.gz\"\n",
    "print (MNIST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(MNIST_FILE, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " torch.Size([50000, 784]),\n",
       " tensor([5, 0, 4,  ..., 8, 4, 8]),\n",
       " torch.Size([50000]),\n",
       " tensor(0),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "n,c = x_train.shape\n",
    "x_train, x_train.shape, y_train, y_train.shape, y_train.min(), y_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "img = x_train[0]\n",
    "plt.imshow(img.view((28,28)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the above into functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    with gzip.open(MNIST_FILE, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How fast is matrix multiplication in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3,4)\n",
    "b = torch.rand(4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ordinary python way of writing matrix multiplication.  It has a horrible tripple nested for loop! We will use pytorch tricks such as broadcasting to simplify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_1(a,b):\n",
    "    ar,ac = a.shape\n",
    "    br,bc = b.shape\n",
    "    assert ac == br, \"row size, colun size mismatch\"\n",
    "    c = torch.zeros(ar,bc)\n",
    "    for i in range (ar):\n",
    "        for j in range (bc):\n",
    "            for k in range (ac):\n",
    "                c[i][j] += a[i][k]*b[k][j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time matmul_1(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (a)\n",
    "print(b)\n",
    "print(a[1,:])\n",
    "print(b[:,0])\n",
    "print (a[1,:]*b[:,0])\n",
    "print ((a[1,:]*b[:,0]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next version eliminates one of the nested loops by using pytorch .sum that runs considerably faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_2(a,b):\n",
    "    ar,ac = a.shape\n",
    "    br,bc = b.shape\n",
    "    assert ac == br, \"row size, colun size mismatch\"\n",
    "    c = torch.zeros(ar,bc)\n",
    "    for i in range (ar):\n",
    "        for j in range (bc):\n",
    "            c[i][j] = (a[i,:]*b[:,j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time matmul_2(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3,4)\n",
    "b = torch.rand(4,3)\n",
    "print (a)\n",
    "print(b)\n",
    "print (a[1].unsqueeze(-1))\n",
    "c = torch.zeros(3,3)\n",
    "c[1]   = (a[1].unsqueeze(-1) * b).sum(dim=0)\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use full power of broadcasting to eliminate the second for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_3(a,b):\n",
    "    ar,ac = a.shape\n",
    "    br,bc = b.shape\n",
    "    assert ac == br, \"row size, colun size mismatch\"\n",
    "    c = torch.zeros(ar,bc)\n",
    "    for i in range (ar):\n",
    "        c[1]   = (a[1].unsqueeze(-1) * b).sum(dim=0)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time matmul_3(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes pytorch is using BLAS library to increase the speed to C speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final comparison let us compare this to pytorch @ operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time a@b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n",
    "set up some random weights and biases.  We will see later how these will have to be randomized in a better way to get a good std and mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(784,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 bias values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = torch.zeros(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = x_valid[:5]\n",
    "m2 = weights\n",
    "m1.shape,m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 _=matmul_1(m1, m2)\n",
    "%timeit -n 10 _=matmul_2(m1, m2)\n",
    "%timeit -n 10 _=matmul_3(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstein summation notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def einstein_matmul(a,b): return torch.einsum('ik,kj->ij', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 _=einstein_matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit -n 10 _=einstein_matmul(m1,m2)\n",
    "%timeit -n 10 _=m1@m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: @ is fastest because it uses libraries that ultimately compute with C speed or cuda speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We have 24*24 = 784 inputs and we have 10 outputs for the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is used to normalize data\n",
    "def z_score(x,m,s): return (x-m)/s\n",
    "normalize = z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "\n",
    "x_valid_mean = x_valid.mean()\n",
    "x_valid_std = x_valid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x_train_mean, x_train_std,x_valid_mean,x_valid_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, x_train_mean, x_train_std)\n",
    "x_valid = normalize(x_valid, x_valid_mean, x_valid_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x_train_mean, x_train_std,x_valid_mean,x_valid_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ideally we would set this up as a classification problem and use cross entropy loss, but I am following the fast.ai course and this is what they want to do initially.\n",
    "\n",
    "input is 28*28 = 784\n",
    "50 hidden layers\n",
    "1 output\n",
    "\n",
    "So we will go:\n",
    "\n",
    "784 -> 50 -> 1\n",
    "\n",
    "This means we have:\n",
    "Layer 1: input@weights1 + bias1 gives us a 50x1 vector.  Input is 1x786 We see that weights must be 786x50 and bias1 is 1x50.\n",
    "\n",
    "Activation layer: Before passing to the output layer we have an activation layer.  This can be ReLu\n",
    "\n",
    "Layer 2: We go 50->1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "num_hidden = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = torch.randn(input_size, num_hidden)\n",
    "bias1 = torch.zeros(1,50)\n",
    "weights2 = torch.randn(50,1)\n",
    "bias2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check that the pipeline is set up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.randn(1, input_size)\n",
    "output_layer_1 = input_data @ weights1 + bias1\n",
    "\n",
    "print(\"Layer 1 output shape:\", output_layer_1.shape)\n",
    "\n",
    "output_layer_2 = output_layer_1 @ weights2\n",
    "\n",
    "print(\"Layer 2 output shape:\", output_layer_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaiming init: Properly randomizes the weights to have mean 0 std 1.  You need this because otherwise you will run into vanishing gradients or exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will write the Kaiming initialization here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    y = (x > 0)\n",
    "    y = y.type(torch.FloatTensor)\n",
    "    return x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, weights, bias):\n",
    "    return x@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "inp = torch.randn(1, input_size)\n",
    "weights1 = torch.randn(input_size, num_hidden)\n",
    "bias1 = torch.zeros(1,50)\n",
    "weights2 = torch.randn(50,1)\n",
    "bias2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 784\n",
    "nh = 50\n",
    "# simplified kaiming init / he init\n",
    "weights1 = torch.randn(m,nh)/math.sqrt(m)\n",
    "bias1 = torch.zeros(nh)\n",
    "weights2 = torch.randn(nh,1)/math.sqrt(nh)\n",
    "bias2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inp, weights1, bias1, weights2, bias2):    \n",
    "    lin1_out = lin(inp, weights1, bias1)\n",
    "    relu_out = ReLU(lin1_out)\n",
    "    lin2_out = lin(relu_out, weights2, bias2)\n",
    "    \n",
    "    return lin2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = forward_pass(inp, weights1, bias1, weights2, bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def MSE(target, inp):\n",
    "    return (target - inp).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = x_train\n",
    "preds = forward_pass(inp,weights1,bias1,weights2,bias2)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()\n",
    "MSE(y_train,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass is all about the chain rule.  \n",
    "\n",
    "Assume we have the following architecture:\n",
    "\n",
    "$$\\text{Input} \\rightarrow L_1 \\rightarrow L_2 \\rightarrow \\dots \\rightarrow L_n \\rightarrow \\text Loss$$\n",
    "\n",
    "It suffices to look at a layer $L$.  We assume this is some layer in the middle.  It has an input $x$ which is a vector (more generally a tensor, but let us consider the vector case since the generalizationis trivial.) of size $k$ and output is a vector of size $l$.  We can consider how the loss $f$ changes as a function of the output of $L$.  Note that we can define the loss function to be $F$ to be the composition of all layers after L and the final loss function $f$.  This gives us the ability to view $L$ as the last layer before the loss.  We define the input to $L$ to be $x$. We now want to know how the parameters of $L$ should be adjusted to minimize the loss. Let us compute with a concerete example.  We will take $L$ to be a linear layer with $3$ neurons and output of $2$ values.  To simplify things a bit I will ignore bias although it is easy to include it and it changes nothing.\n",
    "\n",
    "The weight matrix $w$ is then a $3x2$ matrix. The loss with respect to the outputput of $L$ is:\n",
    "\n",
    "$$F = F((f_1(x), f_2(x)))$$\n",
    "\n",
    "Here $f_1(x) = x \\cdot w_1$ where $w_1$ is first row of $w$ and $f_2(x) = x \\cdot w_2$ where $w_2$ is the second row of w.\n",
    "\n",
    "Lets denote $f_1(x)$ as $out_1$ and $f_2(x)$ as $out_2$.  Then the gradient of $F$ is $(\\frac{\\partial F}{\\partial out_1}, \\frac{\\partial F}{\\partial out_2})$.\n",
    "\n",
    "This is used as layers out.g.  \n",
    "\n",
    "$$\\text{out.g} = (\\frac{\\partial F}{\\partial out_1}, \\frac{\\partial F}{\\partial out_2})$$\n",
    "\n",
    "We can now ask what is out.g for the previous layer? This is the gradient of loss wrt output of the previous layer put through $L$.  The output of previous layer is a vector of dim $3$.\n",
    "\n",
    "This is the gradient of the loss wrt to the input into $L$, $x = (x_1, x_2, x_3)$\n",
    "\n",
    "This is $(\\frac{\\partial F}{\\partial x_1}, \\frac{\\partial F}{\\partial x_2}, \\frac{\\partial F}{\\partial x_3})$.\n",
    "\n",
    "Which in turn is:\n",
    "\n",
    "$$\\left(\\frac{\\partial F(f_1(x_1, x_2, x_3), f_2((x_1, x_2, x_3))}{\\partial x_1}, \\frac{\\partial F(f_1(x_1, x_2, x_3), f_2((x_1, x_2, x_3))}{\\partial x_2}, \\frac{\\partial F(f_1(x_1, x_2, x_3), f_2((x_1, x_2, x_3))}{\\partial x_3}\\right )$$\n",
    "\n",
    "Let us write $g(x) = (f_1(x), f_2(x))$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$\\left( out.g \\cdot \\frac{\\partial g(x)}{\\partial x_1}, out.g \\cdot \\frac{\\partial g(x)}{\\partial x_2}, out.g \\cdot \\frac{\\partial g(x)}{\\partial x_3} \\right)$$\n",
    "\n",
    "This can be written in the matrix form.  \n",
    "\n",
    "$\\begin{bmatrix}out.g_1 && out.g_2\\end{bmatrix} \\begin{bmatrix}\\frac{\\partial f_1(x)}{\\partial x_1} && \\frac{\\partial f_1(x)}{\\partial x_2} && \\frac{\\partial f_1(x)}{\\partial x_3}\\\\\\frac{\\partial f_2(x)}{\\partial x_1} && \\frac{\\partial f_2(x)}{\\partial x_2} && \\frac{\\partial f_2(x)}{\\partial x_3}\\end{bmatrix}$\n",
    "\n",
    "Note that $w = \\begin{bmatrix}w_{11} && w_{12}\\\\ w_{21} && w_{22} \\\\ w_{31} && w_{32}  \\end{bmatrix}$\n",
    "\n",
    "Output of the linear layer (apart from the bias) is:\n",
    "\n",
    "$$\\begin{bmatrix} x_1 && x_2 && x_3 \\end{bmatrix} \\begin{bmatrix}w_{11} && w_{12}\\\\ w_{21} && w_{22} \\\\ w_{31} && w_{32}\\end{bmatrix} = (x_1 w_{11} + x_2 w_{12} + x_3 w_{13}, x_1 w_{21} + x_2 w_{22} + x_3 w_{23}) = (f_1(x), f_2(x)) $$\n",
    "\n",
    "We can immediately see that $\\frac{\\partial f_i(x)}{\\partial x_j} = w_{ij}$\n",
    "\n",
    "Hence we arrive at the important result:\n",
    "\n",
    "$$\\text{inp.g} = \\text{out.g} @ w^T$$\n",
    "\n",
    "Some important comments on inp.g:\n",
    "\n",
    "- This inp.g is the out.g of the previous layer and can in turn be used to calculate its inp.g in similar manner.\n",
    "\n",
    "- We calculate the gradients in reverse order always utilizing the next layers inp.g to calculate the inp.g for previous layer.\n",
    "\n",
    "- For linear layer we can calculate the gradients for the weights and bias using the out.g\n",
    "\n",
    "To calculate how to vary the parameters we need to calculate $\\frac{\\partial F}{\\partial w_{ij}} = \\frac{\\partial F}{\\partial out_1}\\frac{\\partial out_1}{\\partial w_{ij}} + \\frac{\\partial F}{\\partial out_2}\\frac{\\partial out_2}{\\partial w_{ij}}$\n",
    "\n",
    "$\\frac{\\partial out_1}{\\partial w_{ij}} = 0$ if $i \\neq 1$ similarly for $\\frac{\\partial out_2}{\\partial w_{ij}}$ \n",
    "\n",
    "Let us consider $w_{1j}$.  Then $\\frac{\\partial F}{\\partial w_{1j}} = \\frac{\\partial F}{\\partial out_1}\\frac{\\partial out_1}{\\partial w_{1j}}$\n",
    "\n",
    "$\\frac{\\partial out_1}{\\partial w_{1j}} = x_j$\n",
    "\n",
    "So $\\frac{\\partial F}{\\partial w_{1j}} = \\frac{\\partial F}{\\partial out_1} \\cdot x_j$ and $\\frac{\\partial F}{\\partial w_{2j}} = \\frac{\\partial F}{\\partial out_2} \\cdot x_j$\n",
    "\n",
    "This means that the $w.g$ matrix looks like:\n",
    "\n",
    "\\begin{bmatrix}x_1 \\text{out}_1.g && x_1 \\text{out}_2.g \\\\ x_2 \\text{out}_1.g && x_2 \\text{out}_2.g \\\\ x_3 \\text{out}_1.g && x_3 \\text{out}_2.g  \\end{bmatrix}\n",
    "\n",
    "This is inp.g broadcast in axis 1 entrywise multiplied by out.g: inp.unsqueeze(-1) * out.g.unsqueeze(1)\n",
    "\n",
    "We can compute bias.g. We need to find $\\frac{\\partial F}{\\partial b_1} = \\frac{\\partial F}{\\partial out_1}\\frac{\\partial out_1}{\\partial b_1}$\n",
    "\n",
    "$\\frac{\\partial out_1}{\\partial b_1} = 1$\n",
    "\n",
    "So  $\\frac{\\partial F}{\\partial b_1} = \\frac{\\partial F}{\\partial out_1}$\n",
    "\n",
    "Similarly $\\frac{\\partial F}{\\partial b_2} = \\frac{\\partial F}{\\partial out_2}$\n",
    "\n",
    "\n",
    "We have the result:\n",
    "\n",
    "$\\text{bias.g} = out.g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo on how to implement w.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_test= torch.randn(1,3)\n",
    "print (\"a_test: \\n\", a_test, \"\\n\")\n",
    "\n",
    "print (\"a_test.unsqueeze(-1):\\n\", a_test.unsqueeze(-1))\n",
    "\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "b_test = torch.rand(1,2)\n",
    "print(\"b_test: \\n\",b_test)\n",
    "print (\"b_test.unsqueeze(1): \\n\", b_test.unsqueeze(1), \"\\n\")\n",
    "\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "print (\"a_test.unsqueeze(-1) * b_test.unsqueeze(1): \\n\", a_test.unsqueeze(-1) * b_test.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .sum(0) can be used to get rid of extra [ ]\n",
    "print (\"(a_test.unsqueeze(-1) * b_test.unsqueeze(1)).sum(0): \\n\", (a_test.unsqueeze(-1) * b_test.unsqueeze(1)).sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the gradient of loss wrt inp\n",
    "def grad_MSE(target, inp):\n",
    "    inp.g = 2. * (inp.squeeze() - target).unsqueeze(-1) / inp.shape[0]\n",
    "    print(\"inp.g\", inp.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_relu(inp, out):\n",
    "    # grad of relu with respect to input activations\n",
    "    inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_lin(inp, out, w, b):\n",
    "    print(\"out.g\", out.g)\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)\n",
    "    print (b.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward_pass(inp, target, weights1, bias1, weights2, bias2):\n",
    "    #forward pass\n",
    "    lin1_out = lin(inp, weights1, bias1)\n",
    "    relu_out = ReLU(lin1_out)\n",
    "    lin2_out = lin(relu_out, weights2, bias2)\n",
    "    #calling all gradients in reverse order\n",
    "    grad_MSE(target, lin2_out)\n",
    "    grad_lin(relu_out, lin2_out, weights2, bias2)\n",
    "#    grad_relu(lin1_out,relu_out)\n",
    "#    grad_lin(inp,lin1_out, weights1, bias1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward_pass(x_train, y_train, weights1, bias1, weights2, bias2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above is bit clumsy.  We need nice classes to wrap everything in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear():\n",
    "    def __init__(self, weights, bias):\n",
    "        self.w = weights\n",
    "        self.b = bias\n",
    "        self.lr = .1\n",
    "    def __call__(self, inp, target):\n",
    "        self.inp = inp\n",
    "        self.target = target\n",
    "        self.out = self.inp @ self.w + self.b\n",
    "        return self.out\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)\n",
    "        # Adjust the weights and biases\n",
    "        self.w -= self.w.g * self.lr\n",
    "        self.b -= self.b.g * self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu():\n",
    "    def __call__(self, inp, target):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)-0.5\n",
    "        return self.out\n",
    "    def backward(self):\n",
    "        self.inp.g = (self.inp > 0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    def __call__(self,inp, target):\n",
    "        self.inp = inp\n",
    "        self.targ = target\n",
    "        self.out = (inp.squeeze() - target).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, layers, loss_function_class):\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function_class()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x, targ)\n",
    "        self.loss = self.loss_function(x, targ)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss_function.backward()\n",
    "        for l in reversed(self.layers): l.backward()\n",
    "    def zero_grad(self):\n",
    "        for l in self.layers:\n",
    "            if hasattr(l, 'w'):\n",
    "                l.w.g.zero_()\n",
    "                l.b.g.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784\n"
     ]
    }
   ],
   "source": [
    "nh = 50\n",
    "\n",
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "\n",
    "n,a = x_train.shape\n",
    "\n",
    "print (n,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.6999e-06), tensor(1.))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_valid = y_train.float(),y_valid.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights and use Kaiming init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified kaiming init / he init\n",
    "weights1 = torch.randn(a,nh)/math.sqrt(a)\n",
    "bias1 = torch.zeros(nh)\n",
    "weights2 = torch.randn(nh,1)/math.sqrt(nh)\n",
    "bias2 = torch.zeros(1)\n",
    "\n",
    "weights1.g,bias1.g,weights2.g,bias2.g = [None]*4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model([linear(weights1,bias1), ReLu(), linear(weights2,bias2)], MSE)\n",
    "\n",
    "x_train = x_train.float()\n",
    "y_train = y_train.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time test the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time loss = m(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time test the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time m.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the improved architecture we want to use cross entropy for classification.  We start by building softmax, negative log loss and computing gradient for cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is to turn the output of the nn into a probaility distribution.  One way to do this is with softmax.  For vector $X = (x_i)$\n",
    "    \n",
    "  $$Sm_i(X) = \\frac{e^{x_i}}{\\sum e^{x_j}} $$\n",
    "  .\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(x):\n",
    "    return x.exp() / x.exp().sum(-1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.nn.Softmax(1)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log softmax is just $\\ln Sm_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_softmax(x):\n",
    "#    return (x.exp() / x.exp().sum(-1, keepdim = True)).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use some exponential tricks to make this more computable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ln Sm_i = x_i - \\ln \\sum e^{x_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def log_softmax(x):\n",
    "#    return x - x.exp().sum(-1, keepdim = True).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common trick to make this more computable is to factor out the max power m to reduce the magnitude of the exponents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ln Sm_i = x_i - \\ln e^m \\sum e^{x_j-m}$$\n",
    "$$\\ln Sm_i = x_i - m + \\ln \\sum e^{x_j-m}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return x - m[:,None] - (x-m[:,None]).exp().sum(-1, keepdim = True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a prediction $p = (p_i)$ and a one hot encoded target $y = (0, ..., 1, ..., 0)$ with 1 occuring in the ith slot. Cross entropy loss is defined as:\n",
    "\n",
    "$$\\text{loss} = -y \\cdot \\ln p$$\n",
    "\n",
    "Of course here $p_i$ is just $Sm_i$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\text{loss} = -\\sum y_i\\ln Sm_i$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\text{loss} = -\\sum_i y_i(x_i + \\ln \\sum_j e^{x_j})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can compute \n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial x_k } = -\\sum_i y_i \\frac{\\partial}{\\partial x_k}\\ln Sm_i$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x_k}\\ln Sm_i = - \\frac{e^{x_k}}{\\sum_j e^{x_j}} = -Sm_k \\text{ when } i \\neq k$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x_k}\\ln Sm_i = 1 - \\frac{e^{x_i}}{\\sum_j e^{x_j}} = 1 - Sm_k \\text{ when } i = k$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial x_k } = \\left(\\sum_{i \\neq k} y_i Sm_k\\right) + y_k(Sm_k - 1)$$\n",
    "\n",
    "Now $y$ is one hot encoded and all $y_i$ are zero except say for some $y_t$.  If $t \\neq k$ then $\\frac{\\partial \\text{loss}}{\\partial x_k }= Sm_k$.  If $t = k$ then $\\frac{\\partial \\text{loss}}{\\partial x_k } = Sm_t - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion on how to implement cross entropy and how to implement the gradient of cross entropy\n",
    "\n",
    "torch has a number of indexing trics that are helpful. Namely we can index one tensor with another.  Consider the following set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sample = torch.randn(2,10)\n",
    "b_sample = torch.randint(0,9,[2])\n",
    "print (a_sample)\n",
    "print(b_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now index a_sample by the indeces of b_sample.  First we list the rows where we want to grab corresponding entries as indexed by b_sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sample[range(b_sample.shape[0]), b_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to writing a cross entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(inp, targ):\n",
    "    lsm = log_softmax(inp)\n",
    "    loss = -lsm[range(targ.shape[0]), targ].mean()     \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cross_entropy(inp, targ):\n",
    "    grad = softmax(inp)\n",
    "    grad[range(targ.shape[0]),targ] -= 1\n",
    "\n",
    "    return grad/targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_entropy_class:\n",
    "    def softmax(self,x):\n",
    "        return torch.nn.Softmax(1)(x)\n",
    "    \n",
    "    def log_softmax(self,x):\n",
    "        m = x.max(-1)[0]        \n",
    "        return self.softmax(x).log()\n",
    "    \n",
    "    def grad_cross_entropy(self):\n",
    "        grad = self.softmax(self.inp)\n",
    "        grad[range(self.targ.shape[0]),self.targ] -= 1\n",
    "        grad /= self.targ.shape[0]\n",
    "        return grad\n",
    "        \n",
    "\n",
    "    \n",
    "    def __call__(self,inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        lsm = self.log_softmax(inp)\n",
    "        self.out = -lsm[range(self.targ.shape[0]), self.targ].mean()    \n",
    "     \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.grad_cross_entropy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test that our cross entropy function is the same as pytorch cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4181)\n",
      "tensor(0.4181)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "a_pred = Variable(torch.randn(1,4))\n",
    "\n",
    "b_target = Variable(torch.LongTensor([3]))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(a_pred, b_target)\n",
    "print(loss)\n",
    "\n",
    "print(cross_entropy(a_pred, b_target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anatomy of the loop:\n",
    "\n",
    "loop for number of epocs\n",
    "- get data batch\n",
    "- run forward pass\n",
    "- evaluate loss\n",
    "- run backward pass\n",
    "- zero gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model for using cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch size\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_valid_mean = x_valid.mean()\n",
    "x_valid_std = x_valid.std()\n",
    "\n",
    "x_train = normalize(x_train, x_train_mean, x_train_std)\n",
    "x_valid = normalize(x_valid, x_valid_mean, x_valid_std)\n",
    "\n",
    "y_train, y_valid = y_train.long(),y_valid.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784\n"
     ]
    }
   ],
   "source": [
    "#layer dimensions\n",
    "nh = 50\n",
    "out = 10\n",
    "n,a = x_train.shape\n",
    "print(n,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified kaiming init / he init\n",
    "weights1 = torch.randn(a,nh)/math.sqrt(a)\n",
    "bias1 = torch.zeros(nh)\n",
    "weights2 = torch.randn(nh,out)/math.sqrt(nh)\n",
    "bias2 = torch.zeros(out)\n",
    "\n",
    "weights1.g,bias1.g,weights2.g,bias2.g = [None]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model([linear(weights1,bias1), ReLu(), linear(weights2,bias2)], cross_entropy_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = x_train[0:bs]\n",
    "y_batch = y_train[0:bs]\n",
    "\n",
    "preds = m(x_batch,y_batch)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0107)\n"
     ]
    }
   ],
   "source": [
    "eps = 10\n",
    "# lr is in the model\n",
    "for epoch in range(eps):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = m(xb,yb)\n",
    "        m.backward()\n",
    "        m.zero_grad()\n",
    "\n",
    "print(m.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9717)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = m(x_valid,y_valid)\n",
    "accuracy(preds,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of the model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a random sample of the validation set and display the images.  Then we will see what the model predicts the images to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_columns = torch.randperm(1024)[:10]\n",
    "random_sample = x_valid[rand_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = random_sample\n",
    "y_batch = y_valid[rand_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(preds, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 7, 4, 5, 9, 2, 2, 6, 7])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To me these look like: \n",
      "1\n",
      "4\n",
      "7\n",
      "4\n",
      "5\n",
      "9\n",
      "2\n",
      "2\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "columns = 4\n",
    "rows = 5\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "for i in range (0, 10):\n",
    "    img = x_batch[i]\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(img.view((28,28)));\n",
    "plt.show()\n",
    "\n",
    "print (\"To me these look like: \")\n",
    "for i in range (0, preds.shape[0]):\n",
    "    print (predictions[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
